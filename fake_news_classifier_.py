# -*- coding: utf-8 -*-
"""Fake News Classifier .ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1JU2DDV4gMjGjhzl0rIlBNgFKOFyTyr5G

### **FAKE NEWS CLASSIFIER (CONSIDERED DATASET FROM KAGGLE)**

About the Dataset:


1.   title: the title of a news article
2.   author: author of the news article
3.   text: the text of the article; could be incomplete
4.   id: unique id for a news article
5.   label: a label that marks whether the news article is real or fake:

1: unreliable
0: reliable
"""

# Importing the Required Libraries
import numpy as np
import pandas as pd
import re
from nltk.corpus import stopwords
from nltk.stem.porter import PorterStemmer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

import nltk
nltk.download('stopwords')

# printing the stopwords in English
print(stopwords.words('english'))

"""Data Preprocessing"""

# loading the dataset to a pandas DataFrame
news_information = pd.read_csv('/content/train.csv')

news_information.shape

news_information.head(10)

# counting the number of missing values in the dataset
news_information.isnull().sum()

# replacing the null values with empty string
news_information = news_information.fillna('')

# merging the author name and news title
news_information['content'] = news_information['author']+' '+news_information['title']

news_information.head(10)

print(news_information['content'])

# separating the data & label
X = news_information.drop(columns='label', axis=1)
Y = news_information['label']

print(X)
print(Y)

"""**STEMMING**

Stemming is the process of reducing words to their base or root form to improve text analysis by treating different forms of a word as a single term.

Example
"compute"
"computer"
"computing"
"computed"

Using the Porter Stemmer, all these words would be reduced to the stem "comput". This allows a search engine to retrieve documents containing any of these words when a user searches for "compute".
"""

port_stem = PorterStemmer()

def stemming(content):
    stemmed_content = re.sub('[^a-zA-Z]',' ',content)
    stemmed_content = stemmed_content.lower()
    stemmed_content = stemmed_content.split()
    stemmed_content = [port_stem.stem(word) for word in stemmed_content if not word in stopwords.words('english')]
    stemmed_content = ' '.join(stemmed_content)
    return stemmed_content

news_information['content'] = news_information['content'].apply(stemming)

print(news_information['content'])

#separating the data and label
X = news_information['content'].values
Y = news_information['label'].values

print(X)

Y.shape

# converting the textual data to numerical data
vectorizer = TfidfVectorizer()
vectorizer.fit(X)

X = vectorizer.transform(X)

print(X)

"""Splitting the dataset to training & test data"""

X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.2, stratify=Y, random_state=2)

"""## Training the Model:
 **1.Logistic Regression**
"""

LR =LogisticRegression()
LR.fit(X_train, Y_train)

"""EVALUATION

"""

# accuracy score on the training data
X_train_prediction = LR.predict(X_train)
training_data_accuracy = accuracy_score(X_train_prediction, Y_train)

print('Accuracy score of the training data : ', training_data_accuracy)

# accuracy score on the test data
X_test_prediction = LR.predict(X_test)
test_data_accuracy = accuracy_score(X_test_prediction, Y_test)

print('Accuracy score of the test data : ', test_data_accuracy)

from sklearn.metrics import accuracy_score ,classification_report

pred_lr=LR.predict(X_test)

LR.score(X_test, Y_test)

print(classification_report(Y_test, pred_lr))



"""# **2. Decision Trees Classification**"""

from sklearn.tree import DecisionTreeClassifier

DT = DecisionTreeClassifier()
DT.fit(X_train, Y_train)

pred_dt = DT.predict(X_test)

DT.score(X_test, Y_test)

print(classification_report(Y_test, pred_dt))



"""**3. Grading Boost Classifier**"""

from sklearn.ensemble import GradientBoostingClassifier

GBC = GradientBoostingClassifier(random_state=0)
GBC.fit(X_train, Y_train)

pred_gbc = GBC.predict(X_test)

GBC.score(X_test, Y_test)

print(classification_report(Y_test, pred_gbc))



"""**4.Random Forest Classifier**"""

from sklearn.ensemble import RandomForestClassifier

RFC = RandomForestClassifier(random_state=0)
RFC.fit(X_train, Y_train)

pred_rfc = RFC.predict(X_test)

RFC.score(X_test, Y_test)

print(classification_report(Y_test, pred_rfc))



"""## **MODEL TESTING**"""

def output_lable(n):
    if n == 0:
        return "Fake News"
    elif n == 1:
        return "Not A Fake News"

X_new = X_test[3]

prediction = LR.predict(X_new)
print(prediction)

if (prediction[0]==0):
  print('The news is Real')
else:
  print('The news is Fake')

def manual_testing(news_information):
     # Assuming X_new is a feature vector created from news_information
    X_new = preprocess_input(news_information)
    pred_LR = LR.predict(X_new)
    pred_DT = DT.predict(X_new)
    pred_GBC = GBC.predict(X_new)
    pred_RFC = RFC.predict(X_new)

    print(pred_LR, pred_DT, pred_GBC, pred_RFC)

    # Example: Using the prediction from Logistic Regression (LR)
    if pred_LR[0] == 0:
        print('The news is Real')
    else:
        print('The news is Fake')

    # Alternatively, aggregate the predictions and use a majority vote:
    predictions = [pred_LR[0], pred_DT[0], pred_GBC[0], pred_RFC[0]]
    majority_vote = max(set(predictions), key=predictions.count)

    if majority_vote == 0:
        print('The news is Real (based on majority vote)')
    else:
        print('The news is Fake (based on majority vote)')